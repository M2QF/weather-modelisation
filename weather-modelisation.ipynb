{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students\n",
    "- GHAITH Sarahnour (M2QF & ENSIIE)\n",
    "- ROISEUX Thomas (M2QF & ENSIIE)\n",
    "\n",
    "# Introduction\n",
    "## Context\n",
    "\n",
    "The goal of this project is to study \n",
    "the temporal evolution of temperature and wind in France, across one year.\n",
    "\n",
    "## Required packages\n",
    "- `pandas` : to manipulate dataframes.\n",
    "- `numpy` : to manipulate arrays.\n",
    "- `matplotlib` : to plot graphs.\n",
    "- `cartopy` : to plot maps.\n",
    "- `IPython` : to display dataframes in Jupyter Notebook.\n",
    "- `scikit-learn` : to use machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import partial\n",
    "from scipy.stats import norm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "scatter = partial(plt.scatter, transform=ccrs.PlateCarree(), cmap=plt.colormaps()[7])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importation\n",
    "### Preparing GPS dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_df = pd.read_csv(\"data/dataGPS.csv\", header=None, sep=\";\")\n",
    "gps_df.columns = [\"ID\", \"Lattitude\", \"Longitude\"]\n",
    "gps_df[\"ID\"] = gps_df[\"ID\"].str.replace(\"TEMP\", \"\")\n",
    "gps_df.set_index(\"ID\", inplace=True)\n",
    "\n",
    "display(gps_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the GPS data from the file `dataGPS.csv` on a map of France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "for x, y in zip(gps_df[\"Lattitude\"], gps_df[\"Longitude\"]):\n",
    "    ax.plot(x, y, \"r+\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing temperature dataframe and wind dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "hours = [datetime(year, 1, 1, 0, 0, 0) + timedelta(hours=i) for i in range(8760)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"data/dataTemp.csv\", header=None, sep=\";\", index_col=0)\n",
    "temp_df.index.name = \"ID\"\n",
    "for key in temp_df.index:\n",
    "    temp_df.rename(index={key: key.replace(\"TEMP\", \"\")}, inplace=True)\n",
    "temp_df.columns = hours\n",
    "\n",
    "\n",
    "display(temp_df.head())\n",
    "\n",
    "\n",
    "wind_df = pd.read_csv(\"data/dataWind.csv\", header=None, sep=\";\", index_col=0)\n",
    "wind_df.index.name = \"ID\"\n",
    "for key in wind_df.index:\n",
    "    wind_df.rename(index={key: key.replace(\"VVENT\", \"\")}, inplace=True)\n",
    "wind_df.columns = hours\n",
    "\n",
    "\n",
    "display(wind_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: weather in Paris\n",
    "We are goinng to study the weather in Paris, the capital of France, as an example.\n",
    "It is located at 48.51° N, 2.21° E.\n",
    "\n",
    "Let's firstplace in on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "x, y = 2.217999, 48.512381\n",
    "ax.plot(x, y, \"r*\", markersize=15)\n",
    "ax.text(x, y, \"Paris\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to plot the evolution of temperature and wind in Paris, across one year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Temperature in Paris\")\n",
    "plt.plot(temp_df.columns, temp_df.iloc[33, :], color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=4, covariance_type=\"full\", random_state=0)\n",
    "gmm.fit(temp_df.iloc[33, :].values.reshape(-1, 1))\n",
    "ll = gmm.score_samples(temp_df.iloc[33, :].values.reshape(-1, 1))\n",
    "\n",
    "ordered_values = np.argsort(temp_df.iloc[33, :])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Temperature in Paris\")\n",
    "plt.hist(temp_df.iloc[33, :], bins=50, color=\"blue\", density=True, label=\"Temperature\")\n",
    "plt.plot(\n",
    "    [temp_df.iloc[33, i] for i in ordered_values],\n",
    "    [np.exp(ll)[i] for i in ordered_values],\n",
    "    color=\"red\",\n",
    "    label=\"Gaussian Mixture Model\",\n",
    ")\n",
    "plt.axis(xmin=np.min(temp_df.iloc[33, :]), xmax=np.max(temp_df.iloc[33, :]), ymin=0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, we used 4 components for the Gaussian Mixture as we have 4 seasons in a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Wind in Paris\")\n",
    "plt.plot(wind_df.columns, wind_df.iloc[33, :], color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Wind in Paris\")\n",
    "plt.hist(wind_df.iloc[33, :], color=\"blue\", label=\"Wind\", density=True)\n",
    "plt.plot(\n",
    "    np.linspace(0, np.max(wind_df.iloc[33, :]), 100),\n",
    "    norm.pdf(\n",
    "        np.linspace(0, np.max(wind_df.iloc[33, :]), 100),\n",
    "        np.mean(wind_df.iloc[33, :]),\n",
    "        np.std(wind_df.iloc[33, :]),\n",
    "    ),\n",
    "    color=\"red\",\n",
    "    label=\"Normal distribution associated\",\n",
    ")\n",
    "plt.axis(xmin=0, xmax=np.max(wind_df.iloc[33, :]))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "## Cities selection\n",
    "We are going to select 3 more cities in France, to study the weather in different regions.\n",
    "\n",
    "We chose:\n",
    "- Strasbourg (48.58° N, 7.75° E);\n",
    "- Nice (43.70° N, 7.26° E);\n",
    "- Brest (48.39° N, 4.48° W);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_point(x: float, y: float, df: pd.DataFrame) -> Dict[str, float | str]:\n",
    "    \"\"\"Get the closest point to the given coordinates in the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        x (float): longitude\n",
    "        y (float): lattitude\n",
    "        df (pd.DataFrame): dataframe with columns Longitude and Lattitude\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float | str]: closest point\n",
    "    \"\"\"\n",
    "    distances = np.sqrt((df[\"Longitude\"] - x) ** 2 + (df[\"Lattitude\"] - y) ** 2)\n",
    "    dic = df.iloc[np.argmin(distances)].to_dict()\n",
    "    dic[\"ID\"] = df.index[np.argmin(distances)]\n",
    "    return dic\n",
    "\n",
    "\n",
    "strasbourg = find_closest_point(48.5734053, 7.7521113, gps_df)\n",
    "print(\"Strasbourg:\", (strasbourg[\"Longitude\"], strasbourg[\"Lattitude\"]))\n",
    "nice = find_closest_point(43.7009358, 7.2683912, gps_df)\n",
    "print(\"Nice:\", (nice[\"Longitude\"], nice[\"Lattitude\"]))\n",
    "brest = find_closest_point(48.390528, -4.486008, gps_df)\n",
    "print(\"Brest:\", (brest[\"Longitude\"], brest[\"Lattitude\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "x, y = 2.217999, 48.512381\n",
    "ax.plot(x, y, \"r*\", markersize=15)\n",
    "ax.text(x, y, \"Paris\")\n",
    "ax.plot(strasbourg[\"Lattitude\"], strasbourg[\"Longitude\"], \"r*\", markersize=15)\n",
    "ax.text(strasbourg[\"Lattitude\"], strasbourg[\"Longitude\"], \"Strasbourg\")\n",
    "ax.plot(nice[\"Lattitude\"], nice[\"Longitude\"], \"r*\", markersize=15)\n",
    "ax.text(nice[\"Lattitude\"], nice[\"Longitude\"], \"Nice\")\n",
    "ax.plot(brest[\"Lattitude\"], brest[\"Longitude\"], \"r*\", markersize=15)\n",
    "ax.text(brest[\"Lattitude\"], brest[\"Longitude\"], \"Brest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to plot the evolution of temperature and wind in these cities, across one year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(f\"Temperature\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Temperature\")\n",
    "for dict, names in zip((strasbourg, nice, brest), (\"Strasbourg\", \"Nice\", \"Brest\")):\n",
    "    wind_id = dict[\"ID\"]\n",
    "    temp_id = dict[\"ID\"]\n",
    "    plt.plot(temp_df.columns, temp_df.loc[temp_id, :], label=names, alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(f\"Wind\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Wind speed\")\n",
    "for dict, names in zip((strasbourg, nice, brest), (\"Strasbourg\", \"Nice\", \"Brest\")):\n",
    "    wind_id = dict[\"ID\"]\n",
    "    temp_id = dict[\"ID\"]\n",
    "    plt.plot(wind_df.columns, wind_df.loc[wind_id, :], label=names, alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "We are going to cluster the cities in France, using the temperature and wind data.\n",
    "We will use 2 different clustering algorithms:\n",
    "- $K$-means;\n",
    "- hierarchical clustering.\n",
    "### $K$-means\n",
    "We will use $K=4$ for the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_wind, k_means_temp = KMeans(n_clusters=4), KMeans(n_clusters=4)\n",
    "k_means_wind.fit(wind_df)\n",
    "k_means_temp.fit(temp_df)\n",
    "\n",
    "classifications = pd.DataFrame(\n",
    "    k_means_wind.predict(wind_df), index=wind_df.index, columns=[\"K Wind\"]\n",
    ")\n",
    "classifications[\"K Temperature\"] = k_means_temp.predict(temp_df)\n",
    "\n",
    "display(classifications.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to plot the clusters on a map, first for the wind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"K Wind\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to do the same for the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"K Temperature\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here notice that the clusters are not exactly the same but are quite close enough.\n",
    "### Hierarchical clustering\n",
    "After using the $K$-means algorithm, we are going to use hierarchical clustering, with the same number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_wind, agg_temp = AgglomerativeClustering(n_clusters=4), AgglomerativeClustering(\n",
    "    n_clusters=4\n",
    ")\n",
    "agg_wind = agg_wind.fit(wind_df)\n",
    "agg_temp = agg_temp.fit(temp_df)\n",
    "\n",
    "classifications[\"A Wind\"] = agg_wind.labels_\n",
    "classifications[\"A Temperature\"] = agg_temp.labels_\n",
    "\n",
    "display(classifications.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to plot the clusters on a map, first for the wind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"A Wind\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"A Temperature\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average clustering\n",
    "We are going to compute the average of the temperature and wind data, for each city, then classify the cities using the average data.\n",
    "### $K$-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = (wind_df + temp_df) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=4)\n",
    "k_means.fit(wind_df)\n",
    "\n",
    "classifications[\"K Average\"] = k_means.predict(averages)\n",
    "\n",
    "display(classifications.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"K Average\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = AgglomerativeClustering(n_clusters=4)\n",
    "k_means.fit(wind_df)\n",
    "\n",
    "classifications[\"A Average\"] = k_means.labels_\n",
    "\n",
    "display(classifications.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"A Average\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Fortunately, this algorithm is considered as \"naive\", as computing the average between wind and temperature is not relevant, and so is the resuling classification.\n",
    "# Wind clustering\n",
    "We are now going to cluster only wind data, but we will use new algorithms.\n",
    "## Raw data\n",
    "Using the raw time series, this was done in the previous part, under the title \"Clustering\".\n",
    "Here is the resulting plot as a reminder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"K Wind\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore notice that the wind categories are not very relevant, and seems related to the different climates in France.\n",
    "## Principal component analysis\n",
    "We will now perform a principal component analysis on the wind data, to reduce the dimension of the data.\n",
    "We have $n=259$ cities and $p=8460$ time stamps.\n",
    "Our goal is to reduce the dimension of the data, while keeping the most relevant information.\n",
    "Each time stamp is separated by only one hour, so we can assume that the data is highly correlated.\n",
    "Keeping all the time stamps would be redundant, so we are going to reduce the dimension of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = wind_df.corr()\n",
    "corr_matrix_diff = corr_matrix.diff(axis=1).dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have features that describes the wind hour per hour, we will evaluate the differences of the correlation between the features, as it will give a good idea about the correlatoin between two consecutive time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation matrix differences mean:\", corr_matrix_diff.mean().mean())\n",
    "print(\"Correlation matrix differences std:\", corr_matrix_diff.std().std())\n",
    "print(\"Correlation matrix differences min:\", corr_matrix_diff.abs().min().min())\n",
    "print(\"Correlation matrix differences max:\", corr_matrix_diff.abs().max().max())\n",
    "print(\"Correlation matrix differences median:\", corr_matrix_diff.median().median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical information of the correlation matrix is given just above: this shows that the features are highly correlated, as the mean of the differences along the columns is close to $0$.\n",
    "Before doing the PCA, we will do another PCA with all the components, to determine how many ones we must keep in order to keep $95\\,\\%$ of the variance.\n",
    "\n",
    "## Computing the best number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200)\n",
    "\n",
    "pca.fit(wind_df)\n",
    "\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "cumulative_variance_ratio = np.cumsum(variance_ratio)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Variance ratio\")\n",
    "plt.plot(cumulative_variance_ratio)\n",
    "plt.xlabel(\"Principal components number\")\n",
    "plt.ylabel(\"Amount of explained variance\")\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"Minimum number of components to explain 90% of the variance:\",\n",
    "    np.argmax(cumulative_variance_ratio > 0.90) + 1,\n",
    ")\n",
    "print(\n",
    "    \"Minimum number of components to explain 95% of the variance:\",\n",
    "    np.argmax(cumulative_variance_ratio > 0.95) + 1,\n",
    ")\n",
    "print(\n",
    "    \"Minimum number of components to explain 99% of the variance:\",\n",
    "    np.argmax(cumulative_variance_ratio > 0.99) + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to keep at least $95\\,\\%$ of the variance, we will keep the first $k=38$ principal components.\n",
    "As reminder, we had $p=8460$ features, so we reduced the dimension of the data by a factor of $223$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and clustering\n",
    "We are now going to do the PCA with $k=10$ components, and then cluster the data using $K$-means and hierarchical clustering.\n",
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(wind_df)\n",
    "\n",
    "reduced_wind_df = pd.DataFrame(pca.transform(wind_df), index=wind_df.index)\n",
    "display(reduced_wind_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_corr_matrix = reduced_wind_df.corr()\n",
    "display(reduced_corr_matrix)\n",
    "print(\"Reduced correlation matrix mean:\", reduced_corr_matrix.mean().mean())\n",
    "print(\"Reduced correlation matrix std:\", reduced_corr_matrix.std().std())\n",
    "print(\"Reduced correlation matrix min:\", reduced_corr_matrix.abs().min().min())\n",
    "print(\"Reduced correlation matrix max:\", reduced_corr_matrix.abs().max().max())\n",
    "print(\"Reduced correlation matrix median:\", reduced_corr_matrix.median().median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous results shows that all features are now decorrelated, as the mean of the differences along the columns is close to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $K$-means\n",
    "We will use $K=4$ for the number of clusters.\n",
    "\n",
    "We are going to do the same classification as before but now, using the PCA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_k_means_wind = KMeans(n_clusters=4)\n",
    "reduced_k_means_wind.fit(reduced_wind_df)\n",
    "\n",
    "classifications[\"PCA-K Wind\"] = reduced_k_means_wind.predict(reduced_wind_df)\n",
    "\n",
    "display(classifications.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot the clusters on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"PCA-K Wind\"],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a better classification of the wind in France, as the clusters are more homogeneous.\n",
    "Let's try to do the same with hierarchical clustering.\n",
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_ac = AgglomerativeClustering(n_clusters=4)\n",
    "reduced_ac.fit(reduced_wind_df)\n",
    "\n",
    "classifications[\"PCA-A Wind\"] = reduced_ac.labels_\n",
    "\n",
    "display(classifications.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"PCA-A Wind\"],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be a better classification than the previous one, as the clusters are more homogeneous are even more correlated with the different climate zones in France.\n",
    "## Conclusion\n",
    "The last model seems to be the closest one to the reality, as the clusters are more homogeneous and more correlated with the different climate zones in France.\n",
    "This confirms that the PCA was a good way to reduce the dimension of the data, while keeping the most relevant information.\n",
    "# Temperature clustering\n",
    "This part is similar to the previous one, but we will use temperature data instead of wind data.\n",
    "## Raw data\n",
    "Using the raw time series, this was done in the part I, under the title \"Clustering\".\n",
    "Here are the resulting plots as a reminder.\n",
    "### $K$-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"K Temperature\"],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"A Temperature\"],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "These models don't seem very accurate, as close cities along the Mediterrean coast are in different clusters.\n",
    "## Feature engineering\n",
    "Here, we are going to perform a functional principal component analysis on the temperature data, to reduce the dimension of the data.\n",
    "Before doing it, let's write a few words about this method.\n",
    "\n",
    "The fPCA is a generalization of the PCA, as it is used to analyse data that are functions.\n",
    "It is used to analyse the variability of the data, and to reduce the dimension of the data.\n",
    "\n",
    "The fPCA is based on the Karhunen-Loève expansion, which is a generalization of the Fourier series.\n",
    "To perform this analysis, we will use the [`fdasrsf`](https://fdasrsf-python.readthedocs.io/en/latest/fpca_example.html) package, which is a Python package that implements the fPCA.\n",
    "This package needs a number of components to perform the analysis: we are therefore going to compute the best number of components to keep, in order to keep $95\\,\\%$ of the variance.\n",
    "\n",
    "fPCA can be done by 3 different ways:\n",
    "- vertical fPCA, which applies on the time;\n",
    "- horizontal fPCA, which applies on the subjects;\n",
    "- joint fPCA, which applies on both.\n",
    "\n",
    "In our case, as we want to reduce the dimension of the data, we will use the vertical fPCA.\n",
    "\n",
    "To determine the best number of components to keep, we will use the elbow method.\n",
    "Fortuneately, the `fdasrsf` package has a function that does it for us: we just have to supply the argument `var_exp=0.95` o the function `calc_fpca` and it will return the fPCA with the best number of components to keep $95\\,\\%$ of the variance.\n",
    "\n",
    "However, due to package restrictions (quite hard to install on Windows) and high computing time (even using an Azure Linux VM with 8 cores and 16 GB of RAM, optimized for computation), after 9 hours of computing, only 2 components were computed.\n",
    "\n",
    "We then will do a regular PCA to reduce the dimension of the data, and so we are going to apply the same process as before.\n",
    "Let's first see the correlation between contiguous timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = temp_df.corr()\n",
    "corr_matrix_diff = corr_matrix.diff(axis=1).dropna(axis=1)\n",
    "\n",
    "print(\"Correlation matrix differences mean:\", corr_matrix_diff.mean().mean())\n",
    "print(\"Correlation matrix differences std:\", corr_matrix_diff.std().std())\n",
    "print(\"Correlation matrix differences min:\", corr_matrix_diff.abs().min().min())\n",
    "print(\"Correlation matrix differences max:\", corr_matrix_diff.abs().max().max())\n",
    "print(\"Correlation matrix differences median:\", corr_matrix_diff.median().median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding this results, we will apply the same process as before to determine the optimal number of components to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200)\n",
    "\n",
    "pca.fit(wind_df)\n",
    "\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "cumulative_variance_ratio = np.cumsum(variance_ratio)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Variance ratio\")\n",
    "plt.plot(cumulative_variance_ratio)\n",
    "plt.xlabel(\"Principal components number\")\n",
    "plt.ylabel(\"Amount of explained variance\")\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"Minimum number of components to explain 90% of the variance:\",\n",
    "    np.argmax(cumulative_variance_ratio > 0.90) + 1,\n",
    ")\n",
    "print(\n",
    "    \"Minimum number of components to explain 95% of the variance:\",\n",
    "    np.argmax(cumulative_variance_ratio > 0.95) + 1,\n",
    ")\n",
    "print(\n",
    "    \"Minimum number of components to explain 99% of the variance:\",\n",
    "    np.argmax(cumulative_variance_ratio > 0.99) + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results depend on the amount of variance we want to keep.\n",
    "\n",
    "We are now going to keep only the 10 first components, as it is the same number as the one we kept for the wind data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(wind_df)\n",
    "\n",
    "reduced_temp_df = pd.DataFrame(pca.transform(temp_df), index=temp_df.index)\n",
    "display(reduced_temp_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_corr_matrix = reduced_temp_df.corr()\n",
    "display(reduced_corr_matrix)\n",
    "print(\"Reduced correlation matrix mean:\", reduced_corr_matrix.mean().mean())\n",
    "print(\"Reduced correlation matrix std:\", reduced_corr_matrix.std().std())\n",
    "print(\"Reduced correlation matrix min:\", reduced_corr_matrix.abs().min().min())\n",
    "print(\"Reduced correlation matrix max:\", reduced_corr_matrix.abs().max().max())\n",
    "print(\"Reduced correlation matrix median:\", reduced_corr_matrix.median().median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that all features are now decorrelated, as the mean of the differences along the columns is close to $0$.\n",
    "## Model based clustering\n",
    "We are now going to cluster the data using the model based clustering.\n",
    "We will use the Gaussian Mixture model.\n",
    "\n",
    "To do so, we are going to study four different assumptions for the gaussian mixture density:\n",
    "- spherical with equal volume;\n",
    "- spherical with unequal volume;\n",
    "- diagonal, varying volume and shape;\n",
    "- independency between features, varying volume and shape.\n",
    "\n",
    "Our goal is to find the best number of components to use, for each assumption.\n",
    "### Spherical with unequal volume\n",
    "We will use the Bayesian Information Criterion (BIC) to determine the best number of components to use.\n",
    "We will use the `GaussianMixture` class from the `sklearn.mixture` package, which implements the Gaussian Mixture model for a number of components $K$ between 1 and 10 and we will retain the one that minimizes the BIC.\n",
    "\n",
    "The argument `covariance_type` is used to specify the covariance matrix of the gaussian mixture.\n",
    "The value `spherical` means that each component has its own single variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic = []\n",
    "\n",
    "for k in range(2, 30):\n",
    "    gaussian_spherical_equal = GaussianMixture(\n",
    "        n_components=k, covariance_type=\"spherical\", random_state=0\n",
    "    )\n",
    "\n",
    "    gaussian_spherical_equal.fit(reduced_temp_df)\n",
    "\n",
    "    bic.append(gaussian_spherical_equal.bic(reduced_temp_df))\n",
    "\n",
    "print(\"Best number of clusters:\", np.argmin(bic) + 2)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"BIC\")\n",
    "plt.plot(range(2, 30), bic)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"BIC\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this number of components (28), we will now do our model based clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_spherical_equal = GaussianMixture(\n",
    "    n_components=np.argmin(bic) + 2, covariance_type=\"spherical\", random_state=0\n",
    ")\n",
    "\n",
    "labels = gaussian_spherical_equal.fit_predict(reduced_temp_df)\n",
    "\n",
    "classifications[\"GMM - Spherical - Unequal\"] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot them on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"GMM - Spherical - Unequal\"],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acccording to the BIC, we should use $K=28$ components.\n",
    "This leads to a very good classification, as the clusters are very homogeneous and are correlated with the different climate zones in France.\n",
    "However, this number of components is very high, and it might not be very relevant to use it.\n",
    "\n",
    "Let's try to do the same with the other assumptions.\n",
    "\n",
    "### Spherical with equal volume\n",
    "We will do the same as before, but now with the assumption that all components have the same volume.\n",
    "This is done by using the argument `covariance_type='tied'` in the `GaussianMixture` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic = []\n",
    "\n",
    "for k in range(2, 30):\n",
    "    gaussian_spherical_equal = GaussianMixture(\n",
    "        n_components=k, covariance_type=\"tied\", random_state=0\n",
    "    )\n",
    "\n",
    "    gaussian_spherical_equal.fit(reduced_temp_df)\n",
    "\n",
    "    bic.append(gaussian_spherical_equal.bic(reduced_temp_df))\n",
    "\n",
    "print(\"Best number of clusters:\", np.argmin(bic) + 2)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"BIC\")\n",
    "plt.plot(range(2, 30), bic)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"BIC\")\n",
    "plt.show()\n",
    "\n",
    "gaussian_spherical_equal = GaussianMixture(\n",
    "    n_components=np.argmin(bic) + 2, covariance_type=\"tied\", random_state=0\n",
    ")\n",
    "\n",
    "labels = gaussian_spherical_equal.fit_predict(reduced_temp_df)\n",
    "\n",
    "classifications[\"GMM - Spherical - Equal\"] = labels\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"GMM - Spherical - Equal\"],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 clusters provide the best BIC.\n",
    "This is also lower than the previous one, but still very high.\n",
    "\n",
    "## Diagonal, varying volume and shape\n",
    "We will do the same as before, but now with the assumption that the covariance matrix is diagonal, and that the volume and shape of the components can vary.\n",
    "This is done by using the argument `covariance_type='diag'` in the `GaussianMixture` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic = []\n",
    "\n",
    "for k in range(2, 30):\n",
    "    gaussian_diag = GaussianMixture(\n",
    "        n_components=k, covariance_type=\"diag\", random_state=0\n",
    "    )\n",
    "\n",
    "    gaussian_diag.fit(reduced_temp_df)\n",
    "\n",
    "    bic.append(gaussian_diag.bic(reduced_temp_df))\n",
    "\n",
    "print(\"Best number of clusters:\", np.argmin(bic) + 2)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"BIC\")\n",
    "plt.plot(range(2, 30), bic)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"BIC\")\n",
    "plt.show()\n",
    "\n",
    "gaussian_spherical_equal = GaussianMixture(\n",
    "    n_components=np.argmin(bic) + 2, covariance_type=\"diag\", random_state=0\n",
    ")\n",
    "\n",
    "labels = gaussian_spherical_equal.fit_predict(reduced_temp_df)\n",
    "\n",
    "classifications[\"GMM - Diagonal\"] = labels\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"GMM - Diagonal\"],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having only $14$ clusters provides a better classification than the previous one, as the clusters are more homogeneous and are correlated with the different climate zones in France.\n",
    "\n",
    "## Independency between features, varying volume and shape\n",
    "To achieve that, we will use the argument `covariance_type='full'` in the `GaussianMixture` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic = []\n",
    "\n",
    "for k in range(2, 30):\n",
    "    gaussian_diag = GaussianMixture(\n",
    "        n_components=k, covariance_type=\"full\", random_state=0\n",
    "    )\n",
    "\n",
    "    gaussian_diag.fit(reduced_temp_df)\n",
    "\n",
    "    bic.append(gaussian_diag.bic(reduced_temp_df))\n",
    "\n",
    "print(\"Best number of clusters:\", np.argmin(bic) + 2)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"BIC\")\n",
    "plt.plot(range(2, 30), bic)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"BIC\")\n",
    "plt.show()\n",
    "\n",
    "gaussian_spherical_equal = GaussianMixture(\n",
    "    n_components=np.argmin(bic) + 2, covariance_type=\"full\", random_state=0\n",
    ")\n",
    "\n",
    "labels = gaussian_spherical_equal.fit_predict(reduced_temp_df)\n",
    "\n",
    "classifications[\"GMM - Full\"] = labels\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"GMM - Full\"],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we have more clusters, however the classification seems as good as the previous one, because all clusters are homogeneous and are correlated with the different climate zones in France.\n",
    "Also, we know that temperature and winds are continuous variables, so it is more relevant to use this assumption.\n",
    "### Conclusion\n",
    "Among all assumptions, the last two ones seem to be the best ones, as it provides a good classification, with a reasonable number of clusters, and with homogeneous ones that are correlated with the different climate zones in France.\n",
    "## Spectral clustering\n",
    "We are now going to cluster the data using spectral clustering.\n",
    "This method is based on the graph theory, and is used to cluster data that are not linearly separable.\n",
    "We will do this clustering on the fPCA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for k in range(4, 30):\n",
    "    sp_clustering = SpectralClustering(n_clusters=k, n_components=10)\n",
    "    labels = sp_clustering.fit_predict(reduced_temp_df)\n",
    "\n",
    "    silhouette_avg = silhouette_score(reduced_temp_df, labels)\n",
    "\n",
    "    scores.append(silhouette_avg)\n",
    "\n",
    "print(\"Best number of clusters:\", np.argmax(scores) + 4)\n",
    "\n",
    "sp_clustering = SpectralClustering(n_clusters=np.argmax(scores) + 4, n_components=10)\n",
    "sp_clustering.fit(reduced_temp_df)\n",
    "\n",
    "\n",
    "classifications[\"FPCA-Spectral Temperature\"] = sp_clustering.labels_\n",
    "\n",
    "display(classifications.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the spectral clustering results on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    gps_df[\"Lattitude\"],\n",
    "    gps_df[\"Longitude\"],\n",
    "    c=classifications[\"FPCA-Spectral Temperature\"],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering wind and temperature together\n",
    "We are now going to cluster the wind and temperature data together, using the raw time series.\n",
    "## Approach\n",
    "First, we are going to make a dataframe of all wind and temperature data, for each city.\n",
    "\n",
    "We are going to try two different clustering methods:\n",
    "1. The first one will use a PCA then a $K$-means clustering.\n",
    "2. The second one will be made of the same PCA (fPCA cannot be used for the same reasons as before) followed by a spectral clustering.\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df = gps_df.join(wind_df, how=\"inner\").join(\n",
    "    temp_df, how=\"inner\", lsuffix=\"_wind\", rsuffix=\"_temp\"\n",
    ")\n",
    "\n",
    "if aggregated_df.shape[0] != gps_df.shape[0]:\n",
    "    raise RuntimeError(\"Some rows were lost during the join operation\")\n",
    "\n",
    "display(f\"Aggregated DF shape: {aggregated_df.shape}\")\n",
    "display(aggregated_df.head(10))\n",
    "display(aggregated_df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach: PCA and $K$-means\n",
    "### PCA\n",
    "We are going to use the same way as before to determine the number of components to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200)\n",
    "\n",
    "pca.fit(aggregated_df.drop(columns=[\"Lattitude\", \"Longitude\"]))\n",
    "\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "cumulative_variance_ratio = np.cumsum(variance_ratio)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Variance ratio\")\n",
    "plt.plot(cumulative_variance_ratio)\n",
    "plt.xlabel(\"Principal components number\")\n",
    "plt.ylabel(\"Amount of explained variance\")\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"Minimum number of components to explain 90% of the variance:\",\n",
    "    np.argmax(cumulative_variance_ratio > 0.90) + 1,\n",
    ")\n",
    "print(\n",
    "    \"Minimum number of components to explain 95% of the variance:\",\n",
    "    np.argmax(cumulative_variance_ratio > 0.95) + 1,\n",
    ")\n",
    "print(\n",
    "    \"Minimum number of components to explain 99% of the variance:\",\n",
    "    np.argmax(cumulative_variance_ratio > 0.99) + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to explain at least $95\\,\\%$ of the variance, we will keep the first $k=34$ principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=np.argmax(cumulative_variance_ratio > 0.95) + 1)\n",
    "\n",
    "display(f\"{pca=}\")\n",
    "\n",
    "reduced_aggregated_data = pd.DataFrame(\n",
    "    pca.fit_transform(aggregated_df.drop(columns=[\"Lattitude\", \"Longitude\"])),\n",
    "    index=aggregated_df.index,\n",
    ")\n",
    "\n",
    "display(reduced_aggregated_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this new dataframe, we are going to cluster the data using $K$-means.\n",
    "But this time, we will try to find the optimal number of clusters, using the elbow method.\n",
    "This method consists in plotting the inertia of the model, which is the sum of the squared distances between each point and its closest centroid.\n",
    "Then, we will choose the number of clusters where the inertia starts to decrease slowly or stops strongly decreasing.\n",
    "### $K$-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(4, 30)\n",
    "\n",
    "inertia = []\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(reduced_aggregated_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "inertia = np.array(inertia)\n",
    "inertia_ratio = inertia[:-1] / inertia[1:]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(k_values, inertia, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to analyse the differences of the inertia, to determine the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia_ratio = inertia[:-1] / inertia[1:]\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(k_values[1:], inertia_ratio, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Inertia ratio\")\n",
    "plt.title(\"Elbow method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the previous plot, we will choose $K=11$ for the number of clusters.\n",
    "### Plotting the clusters on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_classification = pd.DataFrame(gps_df)\n",
    "\n",
    "k_means = KMeans(n_clusters=11)\n",
    "\n",
    "k_means.fit(reduced_aggregated_data)\n",
    "aggregated_classification[\"K-means\"] = k_means.predict(reduced_aggregated_data)\n",
    "display(aggregated_classification.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    aggregated_classification[\"Lattitude\"],\n",
    "    aggregated_classification[\"Longitude\"],\n",
    "    c=aggregated_classification[\"K-means\"],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to implement the second approach then compare the results.\n",
    "## Second approach: PCA and spectral clustering\n",
    "### PCA\n",
    "We are going to keep the same PCA results as before.\n",
    "### Spectral clustering\n",
    "Before doing the spectral clustering, we will determine the optimal number of clusters.\n",
    "\n",
    "This time, we are going to select the number of clusters that maximizes the silhouette score.\n",
    "\n",
    "The silhouette score is a metric that measures how similar a point is to its own cluster, compared to other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    sp_clustering = SpectralClustering(\n",
    "        n_clusters=k, n_components=np.argmax(cumulative_variance_ratio > 0.95) + 1\n",
    "    )\n",
    "    labels = sp_clustering.fit_predict(reduced_aggregated_data)\n",
    "\n",
    "    silhouette_avg = silhouette_score(reduced_aggregated_data, labels)\n",
    "\n",
    "    scores.append(silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to plot the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Silhouette score\")\n",
    "plt.plot(k_values, scores, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best number of clusters is:\", np.argmax(scores) + 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this plot, we will choose the best number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_clustering = SpectralClustering(\n",
    "    n_clusters=np.argmax(scores) + 4,\n",
    "    n_components=np.argmax(cumulative_variance_ratio > 0.95) + 1,\n",
    ")\n",
    "sp_clustering.fit(reduced_aggregated_data)\n",
    "\n",
    "aggregated_classification[\"Spectral\"] = sp_clustering.labels_\n",
    "display(aggregated_classification.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of cities in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(aggregated_classification[\"Spectral\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([-5, 9, 42, 52])\n",
    "ax.set_title(\"France\")\n",
    "ax.stock_img()\n",
    "\n",
    "scatter(\n",
    "    aggregated_classification[\"Lattitude\"],\n",
    "    aggregated_classification[\"Longitude\"],\n",
    "    c=aggregated_classification[\"Spectral\"],\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion between the two approaches\n",
    "At first sight, the first approach seems to be the best one, as the clusters are more homogeneous and seems to be related to the different climate zones in France.\n",
    "Also, in the second one, we have silhouettes that are negative, which means that some points are closer to other clusters than their own so the classification is not very relevant.\n",
    "\n",
    "The spectral clustering also seems to provide a map that is not quite correlated with the different climate zones in France, but correlated with somehow France geography (mountains, plains, ...).\n",
    "\n",
    "To go further, we could imagine using a neural network to classify the data, as it is a very powerful tool for classification. This would however require a full implementation using external libraries such as `TensorFlow` or `PyTorch`, which is not the goal of this project.\n",
    "Also, regarding the amount of data, we cannot be sure to have better results than the ones we already have.\n",
    "To speed up the computation, we could use a GPU with `CUDA` to perform the computations, but this would require a lot of time to implement, and we are not sure to have better results than the ones we already have."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
